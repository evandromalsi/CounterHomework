# -*- coding: utf-8 -*-
"""Bismillah_fix_bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pfua9YTnaq-Cuox0CKDcvxF6ajh-U2ni
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install nltk
!pip install emoji
!pip install Sastrawi
!pip install naive_bayes

import pandas as pd
import numpy as np
import re
import string
from sklearn.feature_extraction.text import CountVectorizer
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

# NLP
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from nltk.probability import FreqDist
from nltk.tokenize import word_tokenize
import emoji

# Viz
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
from wordcloud import WordCloud
# %matplotlib inline

# Model Naive Bayes
from sklearn import preprocessing
from sklearn import model_selection
from sklearn import linear_model, metrics
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ParameterGrid

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
uploaded = files.upload()

df.info()
print(f'shape: {df.shape}')
df['sentimen'].value_counts()

df_path = 'flip.tsv'
kamus_alay_path = 'kamus_alay.csv'
df = pd.read_csv(df_path, sep='\t', names=['komentar', 'sentimen'])
df.info()
print(f'shape: {df.shape}')
df.head(10)

df

df = pd.read_csv('cleaned80.csv')
df.head(10)



df.isnull().any()

X = df[['komentar']].values
X[0:5]

X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))
X[0:5]

Y = df[['sentimen']].values
y[0:5]

from sklearn.naive_bayes import GaussianNB 
nvclassifier = GaussianNB()

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size=0.2, random_state=2)

nvclassifier.fit(X_train, Y_train)

X=X.reshape(1, -1)

yhat = nvclassifier.predict(X_test)
yhat[0:50]

print ('Train set: ', X_train.shape, Y_train.shape)
print ('Test set: ', X_test.shape, Y_test.shape)

from sklearn import metrics
print("Train set Accuracy: ", metrics.accuracy_score(Y_train, nvclassifier.predict(X_train)))
print("Test set Accuracy: ", metrics.accuracy_score(Y_test, yhat))

from sklearn.model_selection import cross_val_score
all_accuracies = cross_val_score(estimator=nvclassifier, X=X_train, y=Y_train, cv=10)
print(all_accuracies)

# Cek Data Kosong
df.isnull().sum()

df.empty

# Proporsi Variabel Target
df['sentimen'].value_counts()

# Proporsi Variabel Target
df['komentar'].value_counts()

#Visualisasi data
# VISUALISASI BAR
category = df['sentimen']. value_counts().plot(kind='bar')
group_labels = ['NEGATIF', 'NETRAL', 'POSITIF']
category.set_xticklabels(group_labels)
plt.xticks(rotation=0)
plt.ylabel("Total Sentimen")

# VISUALISASI DONAT
def donut(sizes, ax, angle=90, labels=None,colors=None, explode=None, shadow=None):

    # Plot
    ax.pie(sizes, colors = colors, labels=labels, autopct='%.1f%%', 
           startangle = angle, pctdistance=0.8, explode = explode, 
           wedgeprops=dict(width=0.4), shadow=shadow)

    # Formatting
    plt.axis('equal')  
    plt.tight_layout()

# Plot arguments
sizes = df.sentimen.value_counts()
labels = ['Review Baik', 'Review Netral', 'Review Negatif']
colors = ['lightgreen', 'yellow', 'lightcoral']
explode = (0,0,0)

# Create axes
f, ax = plt.subplots(figsize=(6,4))

# plot donut
donut(sizes, ax, 90, labels, colors=colors, explode=explode, shadow=True)
ax.set_title('Total Sentimen')
plt.show()

count = [count for count in df['sentimen'].value_counts()]
labels = list(df['sentimen'].value_counts().index)

plt.title('Total Sentimen', fontsize = 16)
sns.barplot(x=count, y=labels, data=df)

fig, ax = plt.subplots(figsize = (5, 5))
colors = sns.color_palette('pastel')

count_rating = [count for count in df['sentimen'].value_counts()]
labels_rating = list(df['sentimen'].value_counts().index)

plt.title('Total Sentimen', fontsize = 16)
ax.pie(x=count_rating, labels=labels_rating, autopct = '%1.1f%%', 
       textprops={'fontsize': 14}, colors=colors)
plt.show

# Insert new stop words
old_stopwords = stopwords.words('indonesian')
new_stopwords = ["flip", "transfer", "transaksi", "bintang", "masuk", "uang", "udah", "gk", "ga", "gak", "kirim", "ktp",
                 "aplikasi", "app", "nya", "yg", "ya", "bank", "jenius", "neo", "raya", "tmrw", "syariah", "tdk",
                 "dbs", "line bank", "linebank", "livin", "wokee", "seabank", "jago", "blu", "yng", "saldo", 'akun',
                 'aolikasi', 'apliksix', 'aja', 'apk', 'apps', 'dgn', 'ane', 'sy', 'gua', 'gwa', 'si', "biaya",
                 'smpai', 'bgt', 'banget', 'bangettt', 'tu', 'ama', 'utk', 'udh', 'btw', 'ntar', 'lol', 'proses', 'verifikasi',
                 'ttg', 'emg', 'aj', 'tll', 'sih', 'kalo', 'klo', 'trsa', 'mnrt', 'nih', 'ma', 'dr', 'ajaa',
                 'tp', 'akan', 'bs', 'bikin', 'kta', 'pas', 'pdahl', 'bnyak', 'guys', 'tnx', 'bang', 'nang',
                 'mas', 'amat', 'tjoy', 'hemm', 'haha', 'sllu', 'hrs', 'lanjut', 'bgtu', 'sbnrnya', 'trjadi',
                 'pdhl', 'sm', 'plg', 'skrg', 'ny', 'bca', 'mandiri', 'bri', 'btpn', 'dbs', 'brimo', 
                 "admin", 'gimana', 'dlu', 'jam', 'isi', 'foto', "pakai", "pake", 'refund', 'bukti', 'tf', 'cs', 
                 'kali', 'btn', 'min', 'download', 'blm', 'tau', 'rekening', 'salah', 'dana', 'data', 'buka', 'daftar',
                 'kode', 'unik', 'sya', 'top', 'up', 'chat', 'email', 'nggak', "deh", "kasih"]

new_stopwords = new_stopwords + old_stopwords

kamus_alay = pd.read_csv(kamus_alay_path)

normalize_word_dict = {}
for index, row in kamus_alay.iterrows():
    if row[0] not in normalize_word_dict:
        normalize_word_dict[row[0]] = row[1]

# Pre-processing

character = ['.',',',';',':','-,','...','?','!','(',')','[',']','{','}','<','>','"','/','\'','#','-','@',
             'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',
             'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']

def repeatcharClean(text):
# hapus karakter yang berulang
   for i in range(len(character)):
    charac_long = 5
    while charac_long > 2:
      char = character[i]*charac_long 
      text = text.replace(char,character[i])
      charac_long -= 1
      return text

# Pre-processing

def cleaningText(text):
# hapus karakter yang berulang
   for i in range(len(character)):
    charac_long = 5
    while charac_long > 2:
      char = character[i]*charac_long 
      text = text.replace(char,character[i])
      charac_long -= 1
    # hapus emoji
    text = emoji.demojize(text)
    text = re.sub(':[A-Za-z_-]+:', '', text)
    # hapus emoticon
    text = re.sub(r"([xX;:]'?[dDpPvVoO3)(])", '', text)
    # hapus username dan mentions
    text = re.sub(r'@[A-Za-z0-9]+', '', text)
    text = re.sub(r"@[^\s]+[\s]?", ' ', text)
    # hapus hashtag
    text = re.sub(r'#[A-Za-z0-9]+', '', text)
    text = re.sub(r'#(\S+)', r'\1', text)
    # hapus link
    text = re.sub(r"(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9]+\.[^\s]{2,}|www\.[a-zA-Z0-9]+\.[^\s]{2,})", "", text)
    # hapus angka dan simbol
    text = re.sub(r'[0-9]+', '', text)
    text = re.sub('[^a-zA-Z,.?!]+',' ',text)
    # hapus spasi
    text = re.sub('[ ]+',' ',text)
    text = text.replace('\n', ' ') # replace new line into space
    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations
    text = text.strip(' ') # remove characters space from both left and right text
    return text

def casefoldingText(text): # Converting all the characters in a text into lower case
    text = text.lower() 
    return text

def tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens
    text = word_tokenize(text) 
    #ubah bahasa alay
    text = [normalize_word_dict[term] if term in normalize_word_dict else term for term in text]
    return text

def filteringText(text): # Remove stopwords in a text
    listStopwords = set(new_stopwords)
    filtered = []
    for txt in text:
        if txt not in listStopwords:
            filtered.append(txt)
    text = filtered 
    return text

def stemmingText(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    text = [stemmer.stem(word) for word in text]
    return text

def toSentence(text): # Convert list of words into sentence
    text = ' '.join(word for word in text)
    return text

df['komentar'] =df['komentar'].apply(repeatcharClean)
df['text_clean'] =df['komentar'].apply(repeatcharClean)
df['text_clean'] =df['komentar'].apply(cleaningText)
df['text_clean'] = df['text_clean'].apply(casefoldingText)

df['text_preprocessed'] = df['text_clean'].apply(tokenizingText)
df['text_preprocessed'] = df['text_preprocessed'].apply(filteringText)
df['text_preprocessed'] = df['text_preprocessed'].apply(stemmingText)

df.to_csv('cleaned.csv', index=False)

df = pd.read_csv('cleaned80.csv')
df.head(10)

# Cek Data Kosong
df.isnull().sum()

# Initialize the Label Encoder.
le = LabelEncoder()

# Encode the categories
df['sentimen_enc'] = le.fit_transform(df['sentimen'])

# Display the first five rows again to see the result
df

df['prepro'] = LabelEncoder().fit_transform(df['text_preprocessed'].values)
df

# Positif
positif = df[df['sentimen']=='positive']['text_preprocessed'].str.cat(sep=', ')

# Netral
netral = df[df['sentimen']=='neutral']['text_preprocessed'].str.cat(sep=', ')

# Negatif
negatif = df[df['sentimen']=='negative']['text_preprocessed'].str.cat(sep=', ')

# Initialize the word cloud
wc = WordCloud(width = 500, height = 500, min_font_size = 10, background_color ='white')

# Generate the world clouds for each type of comments
positif_wc = wc.generate(positif)

# plot the world cloud for spam                     
plt.figure(figsize = (5, 5), facecolor = None) 
plt.imshow(positif_wc, interpolation="bilinear") 
plt.axis("off") 
plt.title("Positif Word Cloud")
plt.tight_layout(pad = 0) 
plt.show() 
print('')

# Generate the world clouds for each type of comments
netral_wc = wc.generate(netral)

# plot the world cloud for spam                       
plt.figure(figsize = (5, 5), facecolor = None) 
plt.imshow(netral_wc) 
plt.axis("off")
plt.title("Netral Word Cloud")
plt.tight_layout(pad = 0) 
plt.show() 
print('')


# Generate the world clouds for each type of comments
negatif_wc = wc.generate(negatif)

# plot the world cloud for spam                       
plt.figure(figsize = (5, 5), facecolor = None) 
plt.imshow(negatif_wc) 
plt.axis("off")
plt.title("Negatif Word Cloud")
plt.tight_layout(pad = 0) 
plt.show()

# Select the features and the target
X = df['pre']
y = df['sentimen_enc']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)
print ('Train set: ', X_train.shape, y_train.shape)
print ('Test set: ', X_test.shape, y_test.shape)

# Create the tf-idf vectorizer
vectorizer = TfidfVectorizer()

# First fit the vectorizer with our training set
tfidf_train = vectorizer.fit_transform(X_train)

# Now we can fit our test data with the same vectorizer
tfidf_test = vectorizer.transform(X_test)

# Initialize the Multinomial Naive Bayes classifier
nb = MultinomialNB()

# Fit the model
params = {'alpha': [2]}
multinomial_nb_grid = GridSearchCV(nb, param_grid=params, n_jobs=-1, cv=5, verbose=1)
multinomial_nb_grid.fit(tfidf_train, y_train)
# Predict the labels
y_pred = multinomial_nb_grid.predict(tfidf_test)

# Print the accuracy score
print("Accuracy:",multinomial_nb_grid.score(tfidf_test, X_test))

# Print the Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
fig, ax = plt.subplots(figsize = (4, 4))
sns.heatmap(cm, fmt = 'g', annot = True, cmap="PuBu")
ax.xaxis.set_label_position('top')
ax.xaxis.set_ticks_position('top')
ax.set_xlabel('Prediction', fontsize = 14)
ax.set_xticklabels(['negative (0)', 'neutral (1)', 'positive (2)'])
ax.set_ylabel('Actual', fontsize = 14)
ax.set_yticklabels(['negative (0)', 'neutral (1)', 'positive (2)'])
plt.show()

# Print the Classification Report
cr = classification_report(y_test, y_pred)
print("\n\nClassification Report\n")
print(cr)

